#!/bin/bash

#SBATCH --job-name=BRIDGES
#SBATCH --mail-type=ALL
#SBATCH --mail-user=mheyer@stanford.edu

#SBATCH --time=01:00:00
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --mem=50GB
#SBATCH --cpus-per-task=32
#SBATCH --partition=serc
#SBATCH -o Output/SLURM/slurm-%j.out


# Load modules here
ml load julia               # required for both the data preprocessing and the optimization
ml load gurobi              # required for the optimization
#ml load python/3.9.0        # required for the data preprocessing. Snakemake needs Python >3.5. Activate this line, if your Sherlock uses an older version of Python (check with "python --version").

# Run the data preprocessing pipeline
#snakemake -s "./DataPreprocessing/Snakefile" --cores all --use-conda --conda-frontend conda     # executes the data preprocessing pipeline that creates the input files for the optimization

# Run the optimization model
srun julia run_file.jl
