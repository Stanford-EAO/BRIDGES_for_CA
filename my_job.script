#!/bin/bash

#SBATCH --job-name=BRIDGES
#SBATCH --mail-type=ALL
#SBATCH --mail-user=your_username@stanford.edu

#SBATCH --time=15:00:00
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --mem=50GB
#SBATCH --cpus-per-task=46
#SBATCH --partition=serc


# Load modules here
ml load julia               # required for both the data preprocessing and the optimization
ml load gurobi              # required for the optimization
#ml load python/3.9.0        # required for the data preprocessing. Snakemake needs Python >3.5. Activate this line, if your Sherlock uses an older version of Python (check with "python --version").

# Run the data preprocessing pipeline
#snakemake -s "./DataPreprocessing/Snakefile" --cores all --use-conda --conda-frontend conda     # executes the data preprocessing pipeline that creates the input files for the optimization

# Run the optimization model
srun julia run_file.jl